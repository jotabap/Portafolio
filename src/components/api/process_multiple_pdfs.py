"""
Script para procesar m√∫ltiples PDFs y agregarlos al √≠ndice de Azure AI Search
Detecta autom√°ticamente el tipo de documento y aplica metadata apropiada
"""

import PyPDF2
import requests
import json
import os
import argparse
import logging
from dotenv import load_dotenv

# Configurar logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

load_dotenv()

# Configuraci√≥n de Azure
SEARCH_ENDPOINT = os.environ.get("AZURE_SEARCH_ENDPOINT")
SEARCH_KEY = os.environ.get("AZURE_SEARCH_KEY") 
INDEX_NAME = os.environ.get("AZURE_SEARCH_INDEX")
OPENAI_ENDPOINT = os.environ.get("AZURE_OPENAI_ENDPOINT")
OPENAI_KEY = os.environ.get("AZURE_OPENAI_API_KEY")
EMBEDDING_DEPLOYMENT = os.environ.get("AZURE_OPENAI_DEPLOYMENT_EMBED")
API_VERSION = "2024-02-15-preview"

def extract_text_from_pdf(pdf_path):
    """Extrae texto de PDF"""
    try:
        logger.info(f"Extrayendo texto de PDF: {pdf_path}")
        with open(pdf_path, 'rb') as file:
            reader = PyPDF2.PdfReader(file)
            text = ""
            for page_num, page in enumerate(reader.pages, 1):
                page_text = page.extract_text()
                text += page_text + "\n"
                logger.info(f"P√°gina {page_num}: {len(page_text)} caracteres")
        
        logger.info(f"Total extra√≠do: {len(text)} caracteres")
        return text.strip()
    except Exception as e:
        logger.error(f"Error extrayendo texto de PDF: {e}")
        return None

def create_text_chunks(text, chunk_size=1000, overlap=200):
    """Divide el texto en chunks con overlap"""
    logger.info("Creando chunks de texto...")
    chunks = []
    start = 0
    
    while start < len(text):
        end = start + chunk_size
        chunk = text[start:end]
        
        # Si no es el √∫ltimo chunk, buscar un espacio para cortar limpiamente
        if end < len(text):
            last_space = chunk.rfind(' ')
            if last_space > chunk_size - overlap:
                chunk = chunk[:last_space]
                end = start + last_space
        
        if chunk.strip():  # Solo agregar chunks no vac√≠os
            chunks.append(chunk.strip())
        
        start = end - overlap
        
        if start >= len(text):
            break
    
    logger.info(f"Creados {len(chunks)} chunks")
    return chunks

def detect_document_type(pdf_path, text_sample):
    """Detecta el tipo de documento basado en el nombre del archivo y contenido"""
    filename = os.path.basename(pdf_path).lower()
    
    # Detecci√≥n por nombre de archivo
    if any(keyword in filename for keyword in ['about', 'sobre', 'acerca', 'personal']):
        return "aboutme"
    elif any(keyword in filename for keyword in ['hobby', 'hobbies', 'pasatiempo', 'aficiones']):
        return "hobbies"
    elif any(keyword in filename for keyword in ['cv', 'resume', 'curriculum']):
        return "cv"
    else:
        # An√°lisis de contenido
        text_lower = text_sample.lower()
        
        aboutme_keywords = ['sobre mi', 'acerca de', 'personal', 'personalidad', 'valores', 'filosof√≠a']
        hobby_keywords = ['hobby', 'pasatiempo', 'deporte', 'm√∫sica', 'arte', 'viaje', 'lectura', 'afici√≥n']
        cv_keywords = ['experiencia', 'trabajo', 'empresa', 'proyecto', 'certificaci√≥n', 'educaci√≥n']
        
        aboutme_score = sum(1 for word in aboutme_keywords if word in text_lower)
        hobby_score = sum(1 for word in hobby_keywords if word in text_lower)
        cv_score = sum(1 for word in cv_keywords if word in text_lower)
        
        if aboutme_score >= max(hobby_score, cv_score):
            return "aboutme"
        elif hobby_score > cv_score:
            return "hobbies"
        else:
            return "cv"

def generate_embedding(text):
    """Genera embedding usando Azure OpenAI REST API"""
    try:
        url = f"{OPENAI_ENDPOINT}openai/deployments/{EMBEDDING_DEPLOYMENT}/embeddings?api-version={API_VERSION}"
        
        headers = {
            "Content-Type": "application/json",
            "api-key": OPENAI_KEY
        }
        
        data = {
            "input": text
        }
        
        response = requests.post(url, headers=headers, json=data)
        
        if response.status_code == 200:
            result = response.json()
            return result["data"][0]["embedding"]
        else:
            logger.error(f"Error generando embedding: {response.status_code} - {response.text}")
            return None
            
    except Exception as e:
        logger.error(f"Error en la llamada a embedding API: {e}")
        return None

def upload_to_search_index(documents):
    """Sube documentos al √≠ndice de Azure AI Search"""
    if not documents:
        logger.warning("No hay documentos para subir")
        return False
        
    url = f"{SEARCH_ENDPOINT}/indexes/{INDEX_NAME}/docs/index?api-version=2024-07-01"
    headers = {
        "Content-Type": "application/json",
        "api-key": SEARCH_KEY
    }
    
    payload = {
        "value": documents
    }
    
    try:
        logger.info(f"Subiendo {len(documents)} documentos al √≠ndice...")
        response = requests.post(url, headers=headers, json=payload)
        
        if response.status_code == 200:
            logger.info("‚úÖ Documentos subidos exitosamente")
            return True
        else:
            logger.error(f"‚ùå Error subiendo documentos: {response.status_code}")
            logger.error(f"Respuesta: {response.text}")
            return False
            
    except Exception as e:
        logger.error(f"Error en upload: {e}")
        return False

def get_source_title(doc_type):
    """Obtiene el t√≠tulo de la fuente seg√∫n el tipo de documento"""
    titles = {
        "cv": "John Batista CV 2024",
        "aboutme": "John Batista - Sobre M√≠ 2024", 
        "hobbies": "John Batista - Hobbies y Aficiones 2024"
    }
    return titles.get(doc_type, "John Batista - Documento 2024")

def process_pdf(pdf_path, source_override=None):
    """Procesa un PDF y lo agrega al √≠ndice"""
    logger.info(f"üöÄ Iniciando procesamiento de: {pdf_path}")
    
    # Verificar que el archivo existe
    if not os.path.exists(pdf_path):
        logger.error(f"‚ùå Archivo no encontrado: {pdf_path}")
        return False
    
    # Extraer texto del PDF
    text = extract_text_from_pdf(pdf_path)
    if not text:
        logger.error("‚ùå No se pudo extraer texto del PDF")
        return False
    
    # Detectar tipo de documento
    doc_type = detect_document_type(pdf_path, text[:1000])
    logger.info(f"üìã Tipo de documento detectado: {doc_type}")
    
    # Crear chunks del texto
    chunks = create_text_chunks(text)
    if not chunks:
        logger.error("‚ùå No se pudieron crear chunks del texto")
        return False
    
    # Preparar documentos para el √≠ndice
    documents = []
    source_title = source_override or get_source_title(doc_type)
    
    logger.info("üîÑ Generando embeddings...")
    
    for i, chunk in enumerate(chunks, 1):
        logger.info(f"Procesando chunk {i}/{len(chunks)}")
        
        # Generar embedding para el chunk
        embedding = generate_embedding(chunk)
        if not embedding:
            logger.warning(f"‚ö†Ô∏è No se pudo generar embedding para chunk {i}")
            continue
        
        # Crear ID √∫nico para el documento
        filename_base = os.path.splitext(os.path.basename(pdf_path))[0]
        doc_id = f"{doc_type}_{filename_base}_chunk_{i}"
        
        # Crear documento
        document = {
            "@search.action": "upload",
            "id": doc_id,
            "content": chunk,
            "sources": source_title,
            "embeddings": embedding
        }
        
        documents.append(document)
        logger.info(f"‚úÖ Chunk {i} procesado exitosamente")
    
    if not documents:
        logger.error("‚ùå No se procesaron documentos v√°lidos")
        return False
    
    # Subir documentos al √≠ndice
    logger.info(f"üì§ Subiendo {len(documents)} chunks al √≠ndice...")
    success = upload_to_search_index(documents)
    
    if success:
        logger.info(f"üéâ ¬°Procesamiento completado exitosamente!")
        logger.info(f"üìä Estad√≠sticas:")
        logger.info(f"   üìÑ Archivo: {pdf_path}")
        logger.info(f"   üìã Tipo: {doc_type}")
        logger.info(f"   üß© Chunks procesados: {len(documents)}")
        logger.info(f"   üìö Fuente: {source_title}")
        return True
    else:
        logger.error("‚ùå Error subiendo al √≠ndice")
        return False

def main():
    """Funci√≥n principal"""
    parser = argparse.ArgumentParser(description='Procesa PDFs y los agrega al √≠ndice de Azure AI Search')
    parser.add_argument('pdf_path', help='Ruta al archivo PDF')
    parser.add_argument('--source', help='T√≠tulo personalizado para la fuente')
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("üöÄ PROCESADOR DE PDFs PARA PORTFOLIO")
    print("=" * 60)
    print()
    
    # Verificar configuraci√≥n
    missing_config = []
    for key in ["AZURE_SEARCH_ENDPOINT", "AZURE_SEARCH_KEY", "AZURE_SEARCH_INDEX", 
                "AZURE_OPENAI_ENDPOINT", "AZURE_OPENAI_API_KEY", "AZURE_OPENAI_DEPLOYMENT_EMBED"]:
        if not os.environ.get(key):
            missing_config.append(key)
    
    if missing_config:
        logger.error("‚ùå Configuraci√≥n faltante:")
        for key in missing_config:
            logger.error(f"   - {key}")
        return
    
    logger.info("‚úÖ Configuraci√≥n cargada correctamente")
    
    # Procesar PDF
    success = process_pdf(args.pdf_path, args.source)
    
    if success:
        print("\n" + "=" * 60)
        print("‚úÖ ¬°PROCESAMIENTO EXITOSO!")
        print("üîç El documento ya est√° disponible para b√∫squeda en tu chat")
        print("=" * 60)
    else:
        print("\n" + "=" * 60)
        print("‚ùå ERROR EN EL PROCESAMIENTO")
        print("üìù Revisa los logs arriba para m√°s detalles")
        print("=" * 60)

if __name__ == "__main__":
    main()
